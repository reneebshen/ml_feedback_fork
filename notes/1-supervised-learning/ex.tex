\section{Exercises}
\begin{exercise}
Prove Claim~\ref{claim:condition_exp}.
\end{exercise}
\begin{proof}

\noindent
  This can be proven using the tower property of expectation, also known as the law of total expectation. 

\begin{fact}
The Law of Total Expectation states that if $X$ is a random variable whose expected value $\mathbb{E}[X]$ is defined, and $Y$ is any random variable on the same probability space, then $\mathbb{E}[X] = \mathbb{E}[\mathbb{E}[X|Y]]$.
\end{fact}

\noindent
Giving loss $\ell(f(X), Y)$ of a model on a dataset as a random variable with $X$ as another random variable in the same probability space, we can use this fact to expand:

$$
\mathbb{E}_{X, Y \sim \mathcal{D}}[\ell(f(X), Y)]=\mathbb{E}_X[\mathbb{E}_Y[\ell(f(X), Y) \mid X]]
$$


\newthought{For a squared loss} we can plug in $\ell(f(X), Y) = (f(X) - Y)^2$, and we may thus say that we are looking for 
$$\hat{f} = \argmin_f \mathbb{E}_X[\mathbb{E}_Y[(f(X) - Y)^2 \mid X]]$$

\noindent
Since squared loss is convex and the minima of a convex function can be found at the point where the derivative is zero, we may find that:
\begin{align*}
    \frac{d}{df(X)} \mathbb{E}_X[\mathbb{E}_Y[(f(X) - Y)^2 \mid X]] &= 0 \\
    \mathbb{E}_X[\mathbb{E}_Y[2 (\hat{f}(X) - Y) \mid X]] &= 0
\end{align*}

\noindent
By linearity of expectation, this gives us $\hat{f}(X) = \mathbb{E}_Y[Y | X]$
\begin{align*}
    \mathbb{E}_X [\mathbb{E}_Y[2 (\hat{f}(X) - Y) \mid X]] &= 2 (\mathbb{E}_X [\mathbb{E}_Y[\hat{f}(X) \mid X]] - \mathbb{E}_X [\mathbb{E}_Y[Y \mid X]]) \\
    &= 0 \\
    \mathbb{E}_X [\mathbb{E}_Y[\hat{f}(X) \mid X]] &= \mathbb{E}_X [\mathbb{E}_Y[Y \mid X]] \\
    \hat{f}(X) &= \mathbb{E}_X [\mathbb{E}_Y[Y \mid X]]
\end{align*}

% If we only look at the inner expectation and consider our X random variable set to a random value x, we can rewrite our f(X) as f(x) - not a random variable anymore - the variability that we are interested in here is only due to our choice of function in our minimization problem. Let's now just rename this f(x) as a variable z which varies with the function f that we will choose in our minimization problem. Our minimization problem is therefore the following:\\
% $$\hat{z} = \argmin_{z\sim Z} \mathbb{E}_Y[(Z - Y)^2 ]$$

% \noindent
% If we take the derivative over z and find $\hat{z}$ we end up solving:
% $$\mathbb{E}_Y[2(\hat{z} - Y) ] = 0$$
% By linearity of expectation, we get:
% $$\hat{z} = \mathbb{E}[Y]$$
% Coming back to our initial problem, we were looking for the optimal predictor $\hat{f}(x)$.
% Reinjecting x as a variable, our problem ends up being:
% $$\hat{f}(x) = \mathbb{E}[Y \mid X]$$


\newthought{For a 0/1 loss} we can take advantage of the fact that $f(x) \in \{0, 1\}$ to expand the inner expectation:


$$\begin{aligned}
&\mathbb{E}_Y[\ell(0, Y) \mid X=x]=\ell(0,0) \mathbb{P}[Y=0 \mid X=x]+\ell(0,1) \mathbb{P}[Y=1 \mid X=x] \\
&\mathbb{E}_Y[\ell(1, Y) \mid X=x]=\ell(1,0) \mathbb{P}[Y=0 \mid X=x]+\ell(1,1) \mathbb{P}[Y=1 \mid X=x]
\end{aligned}
$$

\noindent
Since it is a 0/1 loss, $\ell(1,0) = \ell(1,0) = 1$ and $\ell(0,0) = \ell(1,1) = 0$ so this expansion simplifies to:

$$\begin{aligned}
&\mathbb{E}_Y[\ell(0, Y) \mid X=x]= \mathbb{P}[Y=1 \mid X=x] \\
&\mathbb{E}_Y[\ell(1, Y) \mid X=x]=\mathbb{P}[Y=0 \mid X=x]
\end{aligned}
$$
We are looking for the predictor that will always predict the smaller of these two terms for a given x (since we are trying to minimize this inner expectation for our overall risk). We want to predict $0$ if $\mathbb{P}[Y=1 \mid X=x] \leq \mathbb{P}[Y=0 \mid X=x]$ and $1$ if $\mathbb{P}[Y=1 \mid X=x] \leq \mathbb{P}[Y=0 \mid X=x]$.

\noindent
We can use this to rewrite:
$$\hat{f}(x) = 1\{\mathbb{P}[Y=1 \mid X=x] \geq \mathbb{P}[Y=0 \mid X=x]\}$$

\noindent
From the property that $Y \in \{0, 1\}$, we can use $\mathbb{P}[Y=1 \mid X=x] + \mathbb{P}[Y=0 \mid X=x] = 1$ to simplify this expression:
$$\hat{f}(x) = 1\{\mathbb{P}[Y=1 \mid X=x] \geq 1 - \mathbb{P}[Y=0 \mid X=x]\}$$ 

\noindent
By the definition of expectation, we simultaneously have:
\begin{align*}
\mathbb{E}[Y \mid X = x] &= 1*\mathbb{P}[Y=1 \mid X=x] + 0*\mathbb{P}[Y=0 \mid X=x] \\
&= \mathbb{P}[Y=1 \mid X=x]
\end{align*}
Putting this all together and setting $t = 1 - \mathbb{P}[Y=0 \mid X=x]\}$ we get:
$$\hat{f}(x) = 1\{\mathbb{E}[Y \mid X = x] \geq t \}$$
\end{proof}

\begin{exercise}
    For the \emph{fixed design} model, show that the excess risk  $R(\theta) = R(\hat{\theta)} - R(\theta_{*}) = \frac{\sigma^{2}d}{n}$.
\end{exercise}
\begin{proof} 
\begin{align*}
\frac{1}{n}\sum_{i=1}^{N}\mathop{{}\mathbb{E}}_{yi}[(\theta^{T}x_{i} - (\theta_{*}^{T}x_{i}+v_{i}))^{2}] &= \frac{1}{n}\sum_{i=1}^{N}\mathop{{}\mathbb{E}}_{yi}[((\theta^{T} - \theta_{*}^{T})x_{i}+v_{i})^{2}] \\
&= \frac{1}{n}\sum_{i=1}^{N}\mathop{{}\mathbb{E}}_{yi}[((\theta^{T} - \theta_{*}^{T})x_{i})^{2}-2(\theta - \theta_{*})^{T}x_{i}v_{i}+v_{i}^{2}] \\
\end{align*}
The terms within the middle of the expectation are independent under a fixed setting and can be reduced to $0$, simplifying it further to \\
\begin{align*}
 \frac{1}{n}\sum_{i=1}^{N}\mathop{{}\mathbb{E}}_{yi}[(\theta^{T}x_{i} - (\theta_{*}^{T}x_{i}+v_{i}))^{2}] = \frac{1}{n}\sum_{i=1}^{N}\mathop{{}\mathbb{E}}_{yi}[((\theta^{T} - \theta_{*}^{T})x_{i})^{2}+\sigma^{2}] 
\end{align*}
Knowing that 
\begin{align*}
    \theta = (x^{T}x)^{-1}x^{T}y = (x^{T}x)^{-1}x^{T}(x\theta_{*}+v)
\end{align*}
The left side of the expectation can be reduced as follows:
\begin{align*}
\mathop{{}\mathbb{E}}||x(\theta-\theta_{*})||_{2}^{2} &=\mathop{{}\mathbb{E}}||x[(x^{T}x)^{-1}x^{T}(x\theta_{*}+v)-\theta_{*}]||_{2}^{2} \\
&=\mathop{{}\mathbb{E}}||x(x^{T}x)^{-1}x^{T}v||_{2}^{2}
\end{align*}
Substituting this back we can further proceed
\begin{align*}
\frac{1}{n}\mathop{{}\mathbb{E}}[v^{T}x(x^{T}x)^{-1}x^{T}x(x^{T}x)^{-1}x^{T}v] &= \frac{1}{n}\mathop{{}\mathbb{E}}[v^{T}x(x^{T}x)^{-1}x^{T}v] \\ 
&= \frac{1}{n}\mathop{{}\mathbb{E}}[v^{T}Pv] \\
&= \frac{1}{n}tr(p)\mathop{{}\mathbb{E}}[vv^{T}] \\ 
&= \frac{\sigma^{2}}{n}tr(p) \\
&= \frac{\sigma^{2}d}{n}
\end{align*}
\end{proof}


\begin{exercise}
For the same fixed design generative model and a new fixed $x_{n+1}$ what is the expected loss $\mathbb{E}_y[(\hat{\theta}^Tx_{n+1}-y_{n+1})^2]$? Can you interpret the quantities?
\end{exercise}
\begin{proof}
\begin{align*}
    \mathbb{E}_y[(\hat{\theta}^Tx_{n+1}-y_{n+1})]
    &=(\hat{\theta}^Tx_{n+1})^2-2\hat{\theta}^Tx_{n+1}\mathbb{E}_y[y_{n+1}]+\mathbb{E}[y_{n+1}^2] \\
    &=(\hat{\theta}^Tx_{n+1})^2-2\hat{\theta}^Tx_{n+1}\theta_{*}^Tx_{n+1}+(\theta_{*}^Tx_{n+1})^2 \\
    &=([\hat{\theta}^T-\theta_{*}^T]x_{n+1})^2\\
    &=([Y^TX(X^TX)^{\dag}-\theta_{*}^T]x_{n+1})^2\\
    &=([(\theta_{*}^TX^T+V^T)X(X^TX)^{\dag}-\theta_{*}^T]x_{n+1})^2\\
    &=(V^TX(X^TX)^{\dag}x_{n+1})^2
\end{align*}
\end{proof}

\begin{exercise}
    Now consider the random design setting which extends the fixed design model by taking each $x_{i}$ to be drawn i.i.d from $\mathcal{N}$($0$, $\Sigma$). 
    We further assume that $v_{i}$ is also Gaussian (and independent of the features).
    The risk is then $R(\theta) = \mathbb{E}_{x,y}[(x^{T}\theta - y)^{2}]$. What is the excess risk of $\hat{\theta}$ in terms of $X^{T}X$ and $\Sigma$? What is the excess risk in terms of $\sigma^{2}, n, d$?
\end{exercise}



