# Week of August 23

This assignment is due EOD on August 30, 2023.
Requirements: Task 0 AND Task 1 AND (Task 2 OR Task 3).

## Submission

As you are working, you can [commit and push](https://docs.github.com/en/get-started/using-git/about-git) to your fork. 
To submit the assignment, you will [create a Pull Request](https://docs.github.com/en/pull-requests/collaborating-with-pull-requests/proposing-changes-to-your-work-with-pull-requests/creating-a-pull-request-from-a-fork) (PR) with the main repository.
Please title the PR "08-23 Submission - netID" and include a description of the work that you did.

## Task 0: Create a personal fork of this repository and a submission python script

First, [fork](https://docs.github.com/en/pull-requests/collaborating-with-pull-requests/working-with-forks) this repository.
Then, create a folder in `code/submission` named `netID`.
Copy the template file `code/instructors/08-23watch_prediction.py` into your folder.

## Task 1: Create and evaluate a linear predictor for video recommendation

The template file includes code for loading [KauiRec](https://kuairec.com/) data, training a linear predictor, and evaluating its performance.
The approach is very basic: it uses only a few features from the available data to predict the target. (The target for each user-item pair is the "watch ratio" - see the documentation for more details.)
See if you can modify the setup or learning algorithm to find a better performing predictor.
Some suggestions: use a richer feature representation, transform or normalize the features or labels, tune the regularization hyperparameters, etc.
Comment your code to explain your additions and report your final performance in your PR.

## (pick one) Task 2: Evaluate your predictor on subgroups

Average performance does not tell the whole story. 
How good is your predictor on subgroups of users or items?
Peruse the KauiRec documentation, and use available user and item information to define subgroups.
Report on the performance of your predictor in these groups. 
If it performs differently, why do you think this is?

Now let's consider a very simple recommendation rule based on thresholding the watch ratios.
For consistency, let's define the true binary label of a user-item pair to be a $0.5$ threshold on the *true* watch ratio. That is, the true binary label of a user-item pair is $1$ if the true watch ratio is at least $0.5$, and $0$ otherwise.
Now turn your predictor into a classifier by thresholding the *predicted* watch ratio.
Evaluate your classifiers with the statistical classification criteria discussed in lecture.
Also investigate the non-discrimination criteria using the same subgroups you defined above.
Include your code and report on your findings in your PR. 
Note: you can attach figures to PRs!

## (pick one) Task 3 (limited capacity): Improve course notes

Select a subsection of the Supervised Learning manuscript chapter and make improvements. 
For example: you could create nice figures (SVGs or tikz), expand and clarify the text, add additional examples, incorporate new material the slides slides, etc.
Use Github Issues to avoid redundancy and scope this task.
