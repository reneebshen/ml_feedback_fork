# Week of August 23

This assignment is due EOD on August 30, 2023.
Requirements: Task 0 AND Task 1 AND (Task 2 OR Task 3).

## Submission

As you are working, you can [commit and push](https://docs.github.com/en/get-started/using-git/about-git) to your fork. 
To submit the assignment, you will [create a Pull Request](https://docs.github.com/en/pull-requests/collaborating-with-pull-requests/proposing-changes-to-your-work-with-pull-requests/creating-a-pull-request-from-a-fork) (PR) with the main repository.
Please title the PR "08-23 Submission - netID" and include a description of the work that you did.

## Task 0: Create a personal fork of this repository and a submission python script

First, [fork](https://docs.github.com/en/pull-requests/collaborating-with-pull-requests/working-with-forks) this repository.
Then, create a folder in `code/submission` named `netID`.
Copy the template file `code/instructors/08-23watch_prediction.py` into your folder.

## Task 1: Create and evaluate a linear predictor for video recommendation

The template file includes code for loading KauiRec data, training a linear classifier, and evaluating its performance.
The approach is very basic: using only a few features from the available data. 
See if you can modify the setup or learning algorithm to find a better performing classifier.
Some suggestions: use a richer feature representation, transform or normalize the features or labels, tune the regularization hyperparameters, etc.
Comment your code to explain your additions and report your final performance in your PR.

## (pick one) Task 2: Evaluate your predictor on subgroups

Average performance does not tell the whole story. 
How good is your predictor on subgroups of users or items?
Peruse the KauiRec documentation and use available user and item information to define subgroups.
Report on the performance of your predictor in these groups. 
If it performs differently, why do you think this is?
Now turn your predictor into a classifier by thresholding the predictions.
You can think of this classifier as a very simple recommendation rule.
Evaluate your classifiers with the statistical classification criteria discussed in lecture.
Also investigate the non-discrimination criteria using the same subgroups you defined above.
Include your code and report on your findings in your PR. 
Note: you can attach figures to PRs!

## (pick one) Task 3 (limited capacity): Improve course notes

Select a subsection of the Supervised Learning manuscript chapter and make improvements. 
For example: you could create nice figures (SVGs or tikz), expand and clarify the text, add additional examples, incorporate new material the slides slides, etc.
Use Github Issues to avoid redundancy and scope this task.
